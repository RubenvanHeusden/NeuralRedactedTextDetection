{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d16806f3",
   "metadata": {},
   "source": [
    "# Evaluation of the neural segmentation models\n",
    "\n",
    "This notebook contains the main results of the paper, where we evaluate the baseline and neural models on both portions of the dataset, and vary the number of training samples for the neural models. We use several evaluation functions from from the `evluation.py` file to perform the evaluation. Note that we have already performed the training and evaluation beforehand, as this can take quite some time, especially on a CPU. Both the training scripts and the trained models are provided if you wish to use these models yourself.\n",
    "\n",
    "## Index\n",
    "\n",
    "1. [Standard Experiment](#standard_experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef05cb4",
   "metadata": {},
   "source": [
    "<a id=\"standard_experiment\"><h2>Standard Experiment</h2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baec69cd",
   "metadata": {},
   "source": [
    "We will start by loading in the the predictions of the models and the ground truth, and do some small visualizations, after which we compare the scores of all the three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9609065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import pycocotools.coco\n",
    "from skimage import measure\n",
    "from torchvision.ops import nms\n",
    "import matplotlib.pyplot as plt\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.mask import decode, encode, merge, iou, area, frPyObjects    \n",
    "\n",
    "# Import the evaluation code\n",
    "from evaluation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20554cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "def load_image(image_path: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function that loads an image from a path.\n",
    "    :param image_path: string specifying the path to the image\n",
    "    :return: Numpy array with the image in BGR format.\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04041e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as F\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = \"tight\"\n",
    "\n",
    "\n",
    "def show(imgs):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86872d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function that can nicely display ground truth and predicted and ground truth masks\n",
    "from torchvision.utils import draw_segmentation_masks\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "\n",
    "from pycocotools.mask import decode, encode, merge, area, frPyObjects, toBbox  \n",
    "\n",
    "def visualize_annotations(image_filename, gold_standard_json, predicted_json, score_treshold: float=0.5,\n",
    "                         vis_mode: str ='masks'):\n",
    "    \n",
    "    assert vis_mode in ['masks', 'boxes']\n",
    "    \n",
    "    plt.rcParams[\"figure.figsize\"] = (20, 20)\n",
    "    image_path = os.path.join(\"/\".join(gold_standard_json.split('/')[:-1]), 'images', image_filename)\n",
    "    im_arr = torch.from_numpy(np.array(load_image(image_path))).permute(2, 0, 1)\n",
    "\n",
    "    with open(gold_standard_json, 'r') as f:\n",
    "        ground_truth_json = json.load(f)\n",
    "        \n",
    "    \n",
    "    with open(predicted_json, 'r') as f:\n",
    "        predicted_json = json.load(f)\n",
    "        \n",
    "    # Add ids to anontations\n",
    "    if isinstance(predicted_json, list):\n",
    "        for i in range(len(predicted_json)):\n",
    "            predicted_json[i]['id'] = i+1\n",
    "    else:\n",
    "        predicted_json = predicted_json['annotations']\n",
    "        \n",
    "    image = [item for item in ground_truth_json['images'] if item['file_name'] == image_filename][0]\n",
    "    image_id = image['id']\n",
    "    scores = {'TP': [], 'FP': [], 'FN': [], 'IOU': []}\n",
    "\n",
    "    ground_truth_annotations = [item for item in ground_truth_json['annotations'] if item['image_id'] == image_id]\n",
    "    predicted_annotations = [item for item in predicted_json if item['image_id'] == image_id]\n",
    "    filtered_predicted_annotations = [item for item in predicted_annotations if area(item['segmentation']) > 0 and (item['score'] > score_treshold)]\n",
    "    print(len(filtered_predicted_annotations))\n",
    "    TP = []\n",
    "    IOU = []\n",
    "\n",
    "    # Next we will have to filter the annotations, as some have an overlap that is too big\n",
    "    # or where the confidence threshold is too low.\n",
    "    ground_truth_indices = [item['id'] for item in ground_truth_annotations]\n",
    "    pred_indices = [item['id'] for item in filtered_predicted_annotations]\n",
    "\n",
    "\n",
    "    for gt_annot in ground_truth_annotations:\n",
    "        for pred_annot in filtered_predicted_annotations:\n",
    "\n",
    "            gt_mask = gt_annot['segmentation']\n",
    "            pred_mask = pred_annot['segmentation']\n",
    "\n",
    "            combined_area = area(merge([gt_mask, pred_mask]))\n",
    "            intersected_area = area(merge([gt_mask, pred_mask], intersect=True))\n",
    "            iou_score = intersected_area/combined_area\n",
    "\n",
    "            if iou_score > 0.5:\n",
    "                TP.append([gt_annot['id'], pred_annot['id']])\n",
    "                IOU.append(iou_score)\n",
    "\n",
    "    FP = set(pred_indices) - set([item[1] for item in TP])\n",
    "    FN = set(ground_truth_indices) - set([item[0] for item in TP])\n",
    "    \n",
    "    decode_func = decode if vis_mode == 'masks' else toBbox\n",
    "    \n",
    "    FP_objects = [decode_func(item['segmentation']) for item in filtered_predicted_annotations if item['id'] in FP]\n",
    "    FN_objects = [decode_func(item['segmentation']) for item in ground_truth_annotations if item['id'] in FN]\n",
    "    TP_objects = [decode_func(item['segmentation']) for item in filtered_predicted_annotations if item['id'] in [it[1] for it in TP]]\n",
    "    \n",
    "    if FP_objects:\n",
    "        FP_objects = np.stack(FP_objects)\n",
    "    if FN_objects:\n",
    "        FN_objects = np.stack(FN_objects)\n",
    "    if TP_objects:\n",
    "        TP_objects = np.stack(TP_objects)\n",
    "    \n",
    "    all_objects = np.concatenate([item for item in [FP_objects, FN_objects, TP_objects] if isinstance(item, np.ndarray)])\n",
    "    colors = ['red' for _ in range(len(FP_objects))] +  ['yellow' for _ in range(len(FN_objects))] + ['green' for _ in range(len(TP_objects))]\n",
    "    if vis_mode == 'masks':\n",
    "        show(draw_segmentation_masks(im_arr, torch.tensor(all_objects).bool(), colors=colors))\n",
    "    else:\n",
    "        all_boxes = box_convert(torch.tensor(all_objects), in_fmt='xywh', out_fmt='xyxy')\n",
    "        show(draw_bounding_boxes(im_arr, all_boxes, colors=colors, width=5))\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfdbdc6",
   "metadata": {},
   "source": [
    "The first step is loading in the classic dataset and then run the evaluation of all three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d306285",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_standard_clean = read_json('../dataset/test/classic_test.json')\n",
    "maskrcnn_clean_predictions = read_json('../model_outputs/MaskRCNN_clean/coco_instances_results.json')\n",
    "mask2former_clean_predictions = read_json('../model_outputs/Mask2Former_clean/coco_instances_results.json')\n",
    "rulebased_clean_predictions = read_json('../model_outputs/Morphology_clean/coco_instances_results.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ed30614",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 281/281 [00:04<00:00, 57.50it/s]\n",
      "100%|█████████████████████████████████████████| 281/281 [02:28<00:00,  1.89it/s]\n",
      "100%|█████████████████████████████████████████| 281/281 [00:03<00:00, 86.85it/s]\n"
     ]
    }
   ],
   "source": [
    "maskrcnn_clean_results = evaluate_predictions(gold_standard_clean['annotations'],\n",
    "                                                             maskrcnn_clean_predictions,\n",
    "                                              confidence_score=0.0, iou_score=0.00,\n",
    "                                             count_empty_pages=False)\n",
    "mask2former_clean_results = evaluate_predictions(gold_standard_clean['annotations'],\n",
    "                                                             mask2former_clean_predictions,\n",
    "                                                confidence_score=0.0, iou_score=0.00,\n",
    "                                                count_empty_pages=False)\n",
    "\n",
    "rulebased_clean_results = evaluate_predictions(gold_standard_clean['annotations'],\n",
    "                                                             rulebased_clean_predictions,\n",
    "                                              do_filtering=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6796cd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classic_results = pd.concat([calculate_color_specific_metrics(rulebased_clean_results)[['black', 'color', 'gray', 'border', 'total']],\n",
    "                             calculate_color_specific_metrics(maskrcnn_clean_results)[['black', 'color', 'gray', 'border', 'total']],\n",
    "                             calculate_color_specific_metrics(mask2former_clean_results)[['black', 'color', 'gray', 'border', 'total']]],\n",
    "                            axis=1,\n",
    "                           keys=[\"Morphology\", \"MaskRCNN\", \"Mask2Former\"]).T[['SQ', 'P', 'R', 'F1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5f214af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SQ': 0.82, 'F1': 0.67, 'P': 0.52, 'R': 0.95}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PQ_calculation(mask2former_clean_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b1cd8b",
   "metadata": {},
   "source": [
    "# Extended dataset results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e211d70f",
   "metadata": {},
   "source": [
    "After these results we will do the other set of experiments, this time on the extended dataset that also contains apges without redactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af5fd7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_standard_extended = read_json('../dataset/test/extended_test.json')\n",
    "maskrcnn_extended_predictions = read_json('../model_outputs/MaskRCNN_extended/coco_instances_results.json')\n",
    "mask2former_extended_predictions = read_json('../model_outputs/Mask2Former_extended/coco_instances_results.json')\n",
    "rulebased_extended_predictions = read_json('../model_outputs/Morphology_extended/coco_instances_results.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "457ffda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 281/281 [00:04<00:00, 61.36it/s]\n",
      "100%|█████████████████████████████████████████| 281/281 [00:04<00:00, 62.98it/s]\n",
      "100%|█████████████████████████████████████████| 281/281 [00:03<00:00, 86.81it/s]\n"
     ]
    }
   ],
   "source": [
    "maskrcnn_extended_results = evaluate_predictions(gold_standard_extended['annotations'],\n",
    "                                                             maskrcnn_extended_predictions,\n",
    "                                              confidence_score=0.7,\n",
    "                                                count_empty_pages=True,\n",
    "                                                iou_score=0.2)\n",
    "mask2former_extended_results = evaluate_predictions(gold_standard_extended['annotations'],\n",
    "                                                             mask2former_extended_predictions,\n",
    "                                                confidence_score=0.7,\n",
    "                                                   count_empty_pages=True,\n",
    "                                                   iou_score=0.2)\n",
    "rulebased_extended_results = evaluate_predictions(gold_standard_extended['annotations'],\n",
    "                                                             rulebased_extended_predictions,\n",
    "                                              do_filtering=False,\n",
    "                                                 count_empty_pages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24b484a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rubenvanheusden/Desktop/IJDARKaj/notebooks/evaluation.py:88: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  R = dataframe['TP'].sum() / (dataframe['TP'].sum() + dataframe['FN'].sum())\n",
      "/Users/rubenvanheusden/Desktop/IJDARKaj/notebooks/evaluation.py:88: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  R = dataframe['TP'].sum() / (dataframe['TP'].sum() + dataframe['FN'].sum())\n",
      "/Users/rubenvanheusden/Desktop/IJDARKaj/notebooks/evaluation.py:88: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  R = dataframe['TP'].sum() / (dataframe['TP'].sum() + dataframe['FN'].sum())\n"
     ]
    }
   ],
   "source": [
    "classic_results = pd.concat([calculate_color_specific_metrics(rulebased_extended_results)[['black', 'color', 'gray', 'border', 'total']],\n",
    "                             calculate_color_specific_metrics(maskrcnn_extended_results)[['black', 'color', 'gray', 'border', 'total']],\n",
    "                             calculate_color_specific_metrics(mask2former_extended_results)[['black', 'color', 'gray', 'border', 'total']]],\n",
    "                            axis=1,\n",
    "                           keys=[\"Morphology\", \"MaskRCNN\", \"Mask2Former\"]).T[['SQ', 'P', 'R', 'F1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03223a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SQ': 0.84, 'F1': 0.9, 'P': 0.92, 'R': 0.88}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PQ_calculation(maskrcnn_extended_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "062f82d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SQ': 0.84, 'F1': 0.87, 'P': 0.94, 'R': 0.81}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PQ_calculation(mask2former_extended_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3259af1a",
   "metadata": {},
   "source": [
    "## Training Variation Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4bb7568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style=\"ticks\")\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "def create_training_df(list_of_dataframes: list):\n",
    "    number_of_samples = [10, 20, 40, 60, 80, 100]\n",
    "    out_df = {}\n",
    "    for i, item in enumerate(list_of_dataframes):\n",
    "        train_metrics = metric_calculation(item)\n",
    "        out_df[number_of_samples[i]] = train_metrics\n",
    "    return pd.DataFrame(out_df).T\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba002e0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mask2former_training' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(nrows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# create_training_df(maskrcnn_training).plot(ax=axes, ylim=(0.6, 1),\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#                                           style=['-', '--', '-.', ':'])\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m create_training_df(\u001b[43mmask2former_training\u001b[49m)\u001b[38;5;241m.\u001b[39mplot(ax\u001b[38;5;241m=\u001b[39maxes, ylim\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.6\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m      6\u001b[0m                                              style\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      8\u001b[0m axes\u001b[38;5;241m.\u001b[39mset_xlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPercentage of training data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39msavefitg(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../mask2former_training.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mask2former_training' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(5, 3))\n",
    "\n",
    "# create_training_df(maskrcnn_training).plot(ax=axes, ylim=(0.6, 1),\n",
    "#                                           style=['-', '--', '-.', ':'])\n",
    "create_training_df(mask2former_training).plot(ax=axes, ylim=(0.6, 1),\n",
    "                                             style=['-', '--', '-.', ':'])\n",
    "\n",
    "axes.set_xlabel(\"Percentage of training data\")\n",
    "plt.savefitg('../mask2former_training.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
