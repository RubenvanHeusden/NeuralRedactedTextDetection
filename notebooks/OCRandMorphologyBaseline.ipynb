{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02c81006",
   "metadata": {},
   "source": [
    "## Experiments of the rulebased model\n",
    "\n",
    "This notebook contains the implementation of the rulebased algorithm as described in [the original paper](https://link.springer.com/chapter/10.1007/978-3-031-43849-3_21), and is largely copied from [the original repository](https://github.com/irlabamsterdam/TPDLTextRedaction). For details on the original implementation we refer to the original repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4410f47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import cv2, os, json, time, pytesseract, platform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from pytesseract import Output\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "# make sure we use the correct path to tesseract when we use windows\n",
    "if platform.system() == 'Windows':\n",
    "    pytesseract.pytesseract.tesseract_cmd = 'C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe'\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f411574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simply load the PNG images\n",
    "def load_image(image_path: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function that loads an image from a path.\n",
    "    :param image_path: string specifying the path to the image\n",
    "    :return: Numpy array with the image in BGR format.\n",
    "    \"\"\"\n",
    "    # Checking if it is an image\n",
    "    if image_path.lower().endswith('.png'):\n",
    "        # Load the image in BGR format\n",
    "        image = cv2.imread(image_path)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daa2769a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(image: np.ndarray, text_pre_closing_kernel_size: tuple = (2, 2),\n",
    "                      text_pre_guassian_blur_size: tuple = (3, 3)) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param image: Numpy array representing the input image in BGR format.\n",
    "    :return: Numpy array with a grayscale image after applied operations.\n",
    "    Method that applies image preprocessing for input to Tesseract, performs the following\n",
    "    operations:\n",
    "    1. Conversion of the iamge to grayscale\n",
    "    2. Closing of the image with a 2 by 2 kernel to remove noise.\n",
    "    3. Guassian blur with a 3 by 3 kernel.\n",
    "    \"\"\"\n",
    "    # First we convert the input image to grayscale\n",
    "    image_grayscale = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # We set up a kernel for the closing operation\n",
    "    kernel = np.ones(text_pre_closing_kernel_size, np.uint8)\n",
    "    \n",
    "    # we perform closing, i.e. dilation followed by erosion\n",
    "    closed_image = cv2.morphologyEx(image_grayscale, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "    # Finally we use a Guassian blur over the image with a 3 by 3 kernel size\n",
    "    image_blurred = cv2.GaussianBlur(closed_image, ksize=text_pre_guassian_blur_size, sigmaX=0)\n",
    "\n",
    "    return image_blurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dabc34cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def redaction_box_preprocessing(image: np.ndarray, box_pre_horizontal_closing_size: tuple = (1, 3),\n",
    "                               box_pre_vertical_closing_size: tuple=(3, 1),\n",
    "                               box_pre_bilat_filter_size: int = 5,\n",
    "                               box_pre_filter_sigma_color: int = 75,\n",
    "                               box_pre_filter_sigma_space: int=75) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    :param image: Numpy array representing the input image in BGR format.\n",
    "    :return: Numpy array with a grayscale image after applied operations.\n",
    "    Method that applies image preprocessing for input the morphological operations, performs the following\n",
    "    operations:\n",
    "    1. Conversion of the iamge to grayscale\n",
    "    2. Horizontal opening with a 1 by 3 kernel\n",
    "    3. Vertical opening with a 3 by 1 kernel\n",
    "    3. Bilateral filter.\n",
    "    \"\"\"\n",
    "    # First we convert the input image to grayscale\n",
    "    image_grayscale = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # We perform two sets of opening operations, a horizontal one, followed by a vertical one.\n",
    "    horizontal_kernel = np.ones(box_pre_horizontal_closing_size, np.uint8)\n",
    "    horizontally_opened_image = cv2.morphologyEx(image_grayscale, cv2.MORPH_OPEN, horizontal_kernel)\n",
    "    \n",
    "    # Apply kernel vertically over the horizontally opened image\n",
    "    vertical_kernel = np.ones(box_pre_vertical_closing_size, np.uint8)\n",
    "    vertically_opened_image = cv2.morphologyEx(horizontally_opened_image, cv2.MORPH_OPEN, vertical_kernel)\n",
    "\n",
    "    # Perform a bilateral blur\n",
    "    bilateral_blurred_image = cv2.bilateralFilter(vertically_opened_image, box_pre_bilat_filter_size,\n",
    "                                                  box_pre_filter_sigma_color, box_pre_filter_sigma_space)\n",
    "\n",
    "    return bilateral_blurred_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4c63555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_text(text_image: np.ndarray, redaction_box_image: np.ndarray,\n",
    "               tesseract_confidence: int = 65):\n",
    "    # Count the total number of pixes of the pages occupied by words\n",
    "    words_area = 0\n",
    "    # Make a copy of the image where we will apply our transformation to.\n",
    "    image_without_text = redaction_box_image.copy()\n",
    "    # Get the width and height of the images\n",
    "    image_height, image_width = redaction_box_image.shape[:2]\n",
    "\n",
    "    # Set up the code to detect the leftmost and rightmost pieces of a page.\n",
    "    left_boundary = [image_width]\n",
    "    right_boundary = [0]\n",
    "    top_boundary = [image_height]\n",
    "    bottom_boundary = [0]\n",
    "\n",
    "    # Specify the codes we want to detect\n",
    "    codes = ['5.1.1.', '5.1.2.']\n",
    "\n",
    "    # run tesseract on the image preprocessed for text\n",
    "    tesseract_output = pytesseract.image_to_data(text_image, lang='nld+eng', output_type=Output.DICT)\n",
    "    \n",
    "    # Get the height of the text\n",
    "    height = np.array(tesseract_output['height'])\n",
    "    # get the median height of the text, we will use this to calculate\n",
    "    # how much of the page is occupied by words\n",
    "    median_text_height = np.median(height[height < 0.3*image_height])\n",
    "\n",
    "    # Get the number of detected text pages\n",
    "    number_of_boxes = len(tesseract_output['level'])\n",
    "    for box in range(number_of_boxes):\n",
    "        # Get the coordinates of the text box if it actually contain any text\n",
    "        if (tesseract_output['text'][box].strip() != \"\") and (tesseract_output['conf'][box] != -1) and (tesseract_output['height'][box] < 0.3*image_height):\n",
    "            (x, y, w, h) = (tesseract_output['left'][box], tesseract_output['top'][box], tesseract_output['width'][box], tesseract_output['height'][box])\n",
    "\n",
    "            # If the text contains one of the codes we want to keep it and not remove it\n",
    "            # from the page\n",
    "            if any([code in tesseract_output['text'][box] for code in codes]):\n",
    "                # If the text is longer we want to adjust the width to include more specific subcodes\n",
    "                if len(tesseract_output['text'][box]) > 7:\n",
    "                    sub_index = tesseract_output['text'][box].find('5.1.')\n",
    "                    char_width = w / len(tesseract_output['text'][box])\n",
    "                    w = int(char_width * 7)\n",
    "                    x += int(char_width * sub_index)\n",
    "                # make the redaction boxes a white color\n",
    "                cv2.rectangle(image_without_text, (x, y), (x + w, y + h), (0, 0, 0), -1)\n",
    "            # If its not a redaction box and the confidence is high enough\n",
    "            # remove the text from hte page\n",
    "            elif tesseract_output['conf'][box] > tesseract_confidence:\n",
    "                words_area += (w*h)\n",
    "                cv2.rectangle(image_without_text, (x, y), (x + w, y + h), (255, 255, 255), -1)\n",
    "\n",
    "                if median_text_height*1.1 > tesseract_output['height'][box] > median_text_height*0.9:\n",
    "                    left_boundary.append(x)\n",
    "                    right_boundary.append(x+w)\n",
    "                    top_boundary.append(y)\n",
    "                    bottom_boundary.append(y+h)\n",
    "\n",
    "    image_without_text = cv2.threshold(image_without_text, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "    \n",
    "    return image_without_text, words_area, {'left_boundary': min(left_boundary),\n",
    "                                'right_boundary': max(right_boundary),\n",
    "                                'top_boundary': min(top_boundary),\n",
    "                               'bottom_boundary': max(bottom_boundary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a96077d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_contours(image_without_text: np.ndarray, contour_opening_kernel_size: tuple = (5, 5)):\n",
    "\n",
    "    # Find the contours we have so far and fill them so we can perform more operations on them\n",
    "    contours = cv2.findContours(image_without_text, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    contours = contours[0] if len(contours) == 2 else contours[1]\n",
    "    for contour in contours:\n",
    "        # filll contours with white\n",
    "        cv2.drawContours(image_without_text, [contour], -1, (255,255,255), -1)\n",
    "\n",
    "    # Here we remove noise by using an opening operation\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, contour_opening_kernel_size)\n",
    "    opened_image = cv2.morphologyEx(image_without_text, cv2.MORPH_OPEN, kernel, iterations=4)\n",
    "\n",
    "    # Draw rectangles\n",
    "    new_contours = cv2.findContours(opened_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    new_contours = new_contours[0] if len(new_contours) == 2 else new_contours[1]\n",
    "    \n",
    "    return opened_image, new_contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "323326bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_contours(original_image: np.ndarray, contours: list, text_boundaries: dict):\n",
    "    \n",
    "    final_image = original_image.copy()\n",
    "    final_contour_image = np.zeros([original_image.shape[0], original_image.shape[1]], dtype=np.uint8)\n",
    "    \n",
    "    # set thresholds for the sizes of the bounding boxes that we are going to keep\n",
    "    area_treshold_min = 0.000125 * original_image.shape[0] * original_image.shape[1]\n",
    "    area_treshold_max = 0.4 * original_image.shape[0] * original_image.shape[1]\n",
    "\n",
    "    left_text_boundary = [text_boundaries['left_boundary']]\n",
    "    right_text_boundary = [text_boundaries['right_boundary']]\n",
    "    top_text_boundary = [text_boundaries['top_boundary']]\n",
    "    bottom_text_boundary = [text_boundaries['bottom_boundary']]\n",
    "    \n",
    "    # boolean indicating if there is any redacted text on the page\n",
    "    redacted_bool = False\n",
    "    number_of_redacted_regions = 0\n",
    "    total_contour_area = 0\n",
    "    # save the final contours\n",
    "    final_contours = []\n",
    "\n",
    "    for contour in contours:\n",
    "        # Find extreme points of contours\n",
    "        contour_left = tuple(contour[contour[:, :, 0].argmin()][0])\n",
    "        contour_right = tuple(contour[contour[:, :, 0].argmax()][0])\n",
    "        contour_top = tuple(contour[contour[:, :, 1].argmin()][0])\n",
    "        contour_bottom = tuple(contour[contour[:, :, 1].argmax()][0])\n",
    "\n",
    "        # Filter out rectangles that are too small, or where the height is bigger than the width\n",
    "        if area_treshold_max > cv2.contourArea(contour) > area_treshold_min and ((contour_bottom[1] - contour_top[1]) < (contour_right[0] - contour_left[0])):\n",
    "            \n",
    "            final_contours.append(contour)\n",
    "            # add the contours into the final image\n",
    "            cv2.drawContours(final_image, [contour], -1, (0,255, 0), thickness=5)\n",
    "            cv2.drawContours(final_contour_image, [contour], -1, (255, 255, 255), -1)\n",
    "\n",
    "            left_text_boundary.append(contour_left[0])\n",
    "            right_text_boundary.append(contour_right[0])\n",
    "            top_text_boundary.append(contour_top[1])\n",
    "            bottom_text_boundary.append(contour_bottom[1])\n",
    "            \n",
    "            total_contour_area += cv2.contourArea(contour)\n",
    "            number_of_redacted_regions += 1\n",
    "\n",
    "    text_area = ((max(right_text_boundary) - min(left_text_boundary)) * (max(bottom_text_boundary) - min(top_text_boundary)))\n",
    "\n",
    "    \n",
    "    return final_image, final_contour_image, final_contours, total_contour_area, text_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3c73bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_algorithm(input_image_path: str,\n",
    "                  text_pre_closing_kernel_size: tuple = (2, 2),\n",
    "                  text_pre_guassian_blur_size: tuple = (3, 3),\n",
    "                  box_pre_horizontal_closing_size: tuple = (1, 3),\n",
    "                  box_pre_vertical_closing_size: tuple = (3, 1),\n",
    "                  box_pre_bilat_filter_size: int = 5,\n",
    "                  box_pre_filter_sigma_color: int = 75,\n",
    "                  box_pre_filter_sigma_space: int = 75,\n",
    "                  tesseract_confidence: int = 65,\n",
    "                  contour_opening_kernel_size: tuple = (5, 5)):\n",
    "    \"\"\"\n",
    "    This functions implements the complete redaction detection algorithm and contains the options\n",
    "    to set the parameters used as to experiment with different settings.\n",
    "    :param input_image_path: string specifying the path to the input image\n",
    "    :param text_pre_closing_kernel_size: size of the closing kernel for the text preprocessing step\n",
    "    :param text_pre_guassian_blur_size: size of the kernel for the Gaussian blur for the text\n",
    "    preprocessing step\n",
    "    :param box_pre_horizontal_closing_size: size of the horizontal closing operation for the redaction \n",
    "    box preprocessing step\n",
    "    :param box_pre_vertical_closing_size:size of the vertical closing operation for the redaction \n",
    "    box preprocessing step\n",
    "    :param box_pre_bilat_filter_size: Size of the bilateral filter kernel for the redaction box\n",
    "    preprocssing step.\n",
    "    :param box_pre_filter_sigma_color: color sigma ofr the bilateral filter of the redaction box\n",
    "    preprocessing step\n",
    "    :param box_pre_filter_sigma_space: space sigma ofr the bilateral filter of the redaction box\n",
    "    preprocessing ste\n",
    "    :param tesseract_confidence: integer specifying the confidence level for Tesseract to \n",
    "    consider something to be text\n",
    "    :param contour_opening_kernel_size: kernel size of the opening operation in the contour detection step.\n",
    "    \"\"\"\n",
    "    \n",
    "    input_image = load_image(input_image_path)\n",
    "    # Do the preprocessing\n",
    "    image_text_pre = text_preprocessing(input_image, text_pre_closing_kernel_size)\n",
    "    \n",
    "    image_box_pre = redaction_box_preprocessing(input_image, \n",
    "                                                box_pre_horizontal_closing_size,\n",
    "                                                box_pre_vertical_closing_size,\n",
    "                                                box_pre_bilat_filter_size,\n",
    "                                                box_pre_filter_sigma_color,\n",
    "                                                box_pre_filter_sigma_space)\n",
    "    # Remove the text\n",
    "    image_without_text, total_words_area, text_boundaries = remove_text(image_text_pre, image_box_pre,\n",
    "                                                                       tesseract_confidence)\n",
    "    # First contour detection step\n",
    "    image_with_contours, contours = determine_contours(image_without_text, contour_opening_kernel_size)\n",
    "    # final contouring filtering step\n",
    "    final_image_with_contours, final_contour_image, final_contours, total_contour_area, total_text_area  = filter_contours(input_image, contours, text_boundaries)\n",
    "    \n",
    "    # Automatically calculate some statistics on the number of redacted boxes, and the total percentage of \n",
    "    # the page that is redacted.\n",
    "    # Check how much of the text area is redacted (%)\n",
    "    percentage_redacted_textarea = ((total_contour_area / total_text_area) * 100) if total_contour_area and total_text_area else 0\n",
    "\n",
    "    # Check how much of character area is redacted (%)\n",
    "    total_area = total_contour_area + total_words_area\n",
    "    percentage_redacted_words = ((total_contour_area / total_area) * 100) if total_contour_area else 0\n",
    "    num_of_redacted_regions = len(final_contours)\n",
    "    \n",
    "    return final_contours, percentage_redacted_words, num_of_redacted_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1dccddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_algorithm_steps(input_image_path):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=7, figsize=(15, 10))\n",
    "    \n",
    "    input_image = load_image(input_image_path)\n",
    "    # Do the preprocessing\n",
    "    image_text_pre, image_box_pre = text_preprocessing(input_image), redaction_box_preprocessing(input_image)\n",
    "    # Remove the text\n",
    "    image_without_text, total_words_area, text_boundaries = remove_text(image_text_pre, image_box_pre)\n",
    "    # First contour detection step\n",
    "    image_with_contours, contours = determine_contours(image_without_text)\n",
    "    # final contouring filtering step\n",
    "    final_image_with_contours, final_contour_image, final_contours, total_contour_area, total_text_area  = filter_contours(input_image, contours, text_boundaries)\n",
    "    \n",
    "    axes[0].imshow(input_image)\n",
    "    axes[1].imshow(image_text_pre, cmap='gray', vmin=0, vmax=255)\n",
    "    axes[2].imshow(image_box_pre, cmap='gray', vmin=0, vmax=255)\n",
    "    axes[3].imshow(image_without_text, cmap='gray', vmin=0, vmax=255)\n",
    "    axes[4].imshow(image_with_contours, cmap='gray', vmin=0, vmax=255)\n",
    "    axes[5].imshow(final_contour_image, cmap='gray', vmin=0, vmax=255)\n",
    "    axes[6].imshow(final_image_with_contours)\n",
    "    \n",
    "    axes[0].set_title(\"Original image\")\n",
    "    axes[1].set_title(\"Text Preprocessing\")\n",
    "    axes[2].set_title(\"Redaction Box\\n Preprocessing\")\n",
    "    axes[3].set_title(\"Text Removal\")\n",
    "    axes[4].set_title(\"Contour Detection\")\n",
    "    axes[5].set_title(\"Contour Filtering\")\n",
    "    axes[6].set_title(\"Final Image \")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9be57bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def read_json(file_name):\n",
    "    with open(file_name, 'r') as json_file:\n",
    "        return json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0236af",
   "metadata": {},
   "source": [
    "## Evaluating the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3a199e",
   "metadata": {},
   "source": [
    "It is relatively simply to run the algorithm and convert the polygons that the model outputs to LRE masks, and use the evaluation fuction the compute the scores for the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f972ebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.mask import frPyObjects, toBbox, area\n",
    "# this function takes the polygons that we have, and convert them to the RLE masks we need.\n",
    "def polygon_to_mask(image, polygons):\n",
    "    annots = []\n",
    "    for polygon in polygons:\n",
    "        points = polygon.flatten()\n",
    "        annots.append(frPyObjects([points], *image.shape[:2])[0])\n",
    "    return annots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f701496",
   "metadata": {},
   "source": [
    "This is all we need to write a function that will allow us to make full predictions on the dataset, and do the evaluation. For ease of use we actually save the results and do the evaluation in the main notebook, to make a complete overview of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0995a485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rulebased_predictions(gold_standard: dict) -> list:\n",
    "    annotations = []\n",
    "    for image in tqdm(gold_standard['images']):\n",
    "        image_name = image['file_name']\n",
    "        fname = os.path.join('../dataset/test/images', image_name)\n",
    "        contours, _, _ = run_algorithm(fname)\n",
    "\n",
    "        image_annotations = polygon_to_mask(cv2.imread(fname), contours)\n",
    "        for annot in image_annotations:\n",
    "            annot['counts'] = annot['counts'].decode()\n",
    "            annotations.append({'bbox': toBbox(annot).tolist(), 'area': int(area(annot)), 'iscrowd': 0,\n",
    "                               'image_id': image['id'], 'segmentation': annot})\n",
    "    return annotations\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a7c8b8",
   "metadata": {},
   "source": [
    "First we just run the regular experiment, and then we run the extended one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4b163f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_standard_clean = read_json('../dataset/test/classic_test.json')\n",
    "gold_standard_extended = read_json('../dataset/test/extended_test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c3745a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|█████████████████████████▉               | 180/284 [07:09<05:47,  3.34s/it]/Users/rubenvanheusden/anaconda3/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/rubenvanheusden/anaconda3/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "100%|█████████████████████████████████████████| 284/284 [11:42<00:00,  2.47s/it]\n"
     ]
    }
   ],
   "source": [
    "classic_output = run_rulebased_predictions(gold_standard_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "70f42117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just save this as a list in json, like the other predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac3e38d",
   "metadata": {},
   "source": [
    "We also run the model on the extended dataset and then we save both for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ab55b4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|████████▋                                 | 91/439 [03:30<15:41,  2.71s/it]/Users/rubenvanheusden/anaconda3/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/rubenvanheusden/anaconda3/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 38%|███████████████▊                         | 169/439 [06:40<11:18,  2.51s/it]/Users/rubenvanheusden/anaconda3/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/rubenvanheusden/anaconda3/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 51%|█████████████████████                    | 225/439 [08:44<08:46,  2.46s/it]/Users/rubenvanheusden/anaconda3/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/rubenvanheusden/anaconda3/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 52%|█████████████████████▎                   | 228/439 [08:50<07:47,  2.21s/it]/Users/rubenvanheusden/anaconda3/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/rubenvanheusden/anaconda3/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 65%|██████████████████████████▌              | 284/439 [10:53<07:19,  2.84s/it]/Users/rubenvanheusden/anaconda3/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/rubenvanheusden/anaconda3/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 67%|███████████████████████████▍             | 294/439 [11:14<05:11,  2.15s/it]/Users/rubenvanheusden/anaconda3/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/rubenvanheusden/anaconda3/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      " 90%|████████████████████████████████████▉    | 396/439 [17:22<01:45,  2.44s/it]/Users/rubenvanheusden/anaconda3/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/rubenvanheusden/anaconda3/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "100%|█████████████████████████████████████████| 439/439 [19:03<00:00,  2.60s/it]\n"
     ]
    }
   ],
   "source": [
    "extended_output = run_rulebased_predictions(gold_standard_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a3b5b9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../model_outputs/Morphology_clean/coco_instances_results.json', 'w') as f:\n",
    "    json.dump(classic_output, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "536db623",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../model_outputs/Morphology_extended/coco_instances_results.json', 'w') as f:\n",
    "    json.dump(extended_output, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
